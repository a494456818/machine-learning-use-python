{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "import numpy as np\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#读取iris数据集,该数据集是根据人口普查数据预测收入是否超过每年50,000美元。\n",
    "data,target =  load_iris(return_X_y=True)\n",
    "\n",
    "#将data和target进行水平拼接\n",
    "target = target.reshape(target.shape[0],1)\n",
    "dataset = np.hstack((data,target))\n",
    "\n",
    "features_name = [\"sepal length in cm \",\"sepal width in cm\",\"petal length in cm\",\"petal width in cm\"]\n",
    "features_columns = [0,1,2,3]\n",
    "labels_name = [\"Iris Setosa\",\"Iris Versicolour\",\"Iris Virginica\"]\n",
    "\n",
    "#计算信息熵\n",
    "def calculate_entropy(data):\n",
    "    length = len(data) #数据集的长度\n",
    "    labels = [i for i in set(data[:,-1])] #标签\n",
    "    #计算每一个标签对应的数据量\n",
    "    labelCounts = {}\n",
    "    for d in data:\n",
    "        label = d[-1]\n",
    "        if label in labelCounts.keys():\n",
    "            labelCounts[label] += 1\n",
    "        else:\n",
    "            labelCounts[label] = 1\n",
    "    #计算熵\n",
    "    ent = 0.0\n",
    "    for key in labelCounts.keys():\n",
    "        class_num = labelCounts[key] #类别为key的数量\n",
    "        prob = class_num/length\n",
    "        ent -= (prob)*math.log(prob,2)\n",
    "    \n",
    "    return ent\n",
    "\n",
    "#根据某一列的取值来划分数据集\n",
    "def splitDataSet(data,column,value,isContinuous,direction):\n",
    "    '''\n",
    "    column:对应是哪一列，column=0表示第一列\n",
    "    value：column列选择值等于的value集合作为子数据集\n",
    "    isContinuous：是否为连续值，如果为True，当direction=-1时，选择左区间及<value的区间，若direction=1，选择右区间\n",
    "    return：column列中为value值时的子数据集\n",
    "    '''\n",
    "    subSet = []\n",
    "    if isContinuous: #如果为连续数据\n",
    "        if direction == 1:\n",
    "            for d in data:\n",
    "                if d[column] <= value:\n",
    "                    subSet.append(d.tolist())\n",
    "                    \n",
    "        elif direction == -1:\n",
    "            for d in data:\n",
    "                if d[column] >= value:\n",
    "                    subSet.append(d.tolist())\n",
    "                    \n",
    "    else:#否则为枚举值\n",
    "        for d in data:\n",
    "            if d[column] == value:\n",
    "                subSet.append(d.tolist())\n",
    "\n",
    "    #转换成ndarray类型\n",
    "    subSet = np.array(subSet)\n",
    "    \n",
    "    return subSet\n",
    "    \n",
    "\n",
    "#从features_columns列中，选择信息增益最大的列所对应的特征\n",
    "def select_best_features(data,features_columns):\n",
    "    '''\n",
    "    features_columns:一个list，表示代表的第features_columns[*]列，从0开始\n",
    "    '''\n",
    "    if calNotNegative1(features_columns) == 1:#如果只有1个可选的特征，则不需要选择了\n",
    "        #查找不为-1所在的列是多少\n",
    "        for features_column in features_columns:\n",
    "            if features_column != -1:\n",
    "                return features_column\n",
    "    \n",
    "    best_feature_column = None #最好的特征所在的列\n",
    "    best_feature_gain = -999999999 #最好的增益率\n",
    "    #计算全部data的信息熵\n",
    "    all_ent = calculate_entropy(data)\n",
    "    #按照特征划分数据集分别进行信息熵\n",
    "    for feature_column in returnNotNegative1List(features_columns):\n",
    "        columns_value_num = set(data[:,feature_column]) #feature_column列中值的个数\n",
    "        if len(columns_value_num)>6:#如果大于6个值，把它当做连续值\n",
    "            #为了简单起见，这里只使用平均值来划分数据集\n",
    "            ave = np.average(data[:,feature_column],axis=0)\n",
    "            #计算信息增益\n",
    "            left_subData = splitDataSet(data,feature_column,ave,isContinuous=True,direction=-1)\n",
    "            right_subData = splitDataSet(data,feature_column,ave,isContinuous=True,direction=1)\n",
    "            left_ent = calculate_entropy(left_subData)\n",
    "            right_ent = calculate_entropy(right_subData)\n",
    "            ent = (len(left_subData)/len(data))*left_ent + (len(right_subData)/len(data))*right_ent\n",
    "            gain = all_ent - ent\n",
    "            if best_feature_gain<gain:\n",
    "                best_feature_gain = gain\n",
    "                best_feature_column = feature_column\n",
    "    return best_feature_column\n",
    "\n",
    "#如果把所有的特征都选择完后，数据的标签仍然不唯一，则采用少数服从多数的投票方式来决定最终的标签\n",
    "def vote(data):\n",
    "    labels = data[:,-1]#该数据集所有的标签\n",
    "    labelDict = {}\n",
    "    for label in labels:\n",
    "        if label in labelDict.keys():\n",
    "            labelDict[label] += 1\n",
    "        else:\n",
    "            labelDict[label] = 1\n",
    "            \n",
    "    max_label = None\n",
    "    for label in labelDict.keys():\n",
    "        if max_label is None:\n",
    "            max_label = label\n",
    "        elif labelDict[label] > labelDict[max_label]:\n",
    "            max_label = label\n",
    "    return max_label\n",
    "\n",
    "#计算一个list中值不为-1的数量\n",
    "def calNotNegative1(list):\n",
    "    count = 0\n",
    "    for i in list:\n",
    "        if i != -1:\n",
    "            count += 1\n",
    "    return count\n",
    "\n",
    "#返回list中不为-1的数\n",
    "def returnNotNegative1List(list):\n",
    "    l = []\n",
    "    for i in list:\n",
    "        if i != -1:\n",
    "            l.append(i)\n",
    "    return l\n",
    "    \n",
    "    \n",
    "\n",
    "def create_tree(data,features_columns):\n",
    "    '''\n",
    "    data：数据集\n",
    "    features：数据集中特征的列号，从0开始，不含标签\n",
    "    '''\n",
    "    #所有数据的类标签相同，属于同一类\n",
    "    labelSet = set(data[-1])\n",
    "    if len(labelSet) == 1:\n",
    "        return labelSet[0]\n",
    "    #已经遍历完所有的特征\n",
    "    if calNotNegative1(features_columns) == 0:\n",
    "        return vote(data)\n",
    "    #找出当前最优的分类特征列号\n",
    "    best_feature_col = select_best_features(data,features_columns)\n",
    "    #使用字典嵌套字典的方法来存放DT的信息\n",
    "    myTree={features_name[best_feature_col]:{}}\n",
    "    \n",
    "    #复制特征列，防止改变原始的特征数据\n",
    "    subFeature_columns = features_columns[:]\n",
    "    subFeature_columns[best_feature_col] = -1 #-1表示删除了一个特征\n",
    "    #获取最优特征中所在列的值\n",
    "    bestFeatureValues=[d[best_feature_col] for d in data]\n",
    "    uniqueValues = set(bestFeatureValues) #取唯一值\n",
    "    if len(uniqueValues)>6:#视为连续值\n",
    "        ave = np.average(data[:,best_feature_col],axis=0)#使用均值作为分割点\n",
    "        for i in [-1,1]:\n",
    "            myTree[features_name[best_feature_col]][(\"<=\" if i==-1 else \">=\")+str(ave)]=create_tree\\\n",
    "            (splitDataSet(data,best_feature_col,ave,isContinuous=True,direction=i),subFeature_columns)\n",
    "            \n",
    "    else:#枚举值的情况\n",
    "        for value in uniqueValues:#遍历枚举值\n",
    "            myTree[features_name[best_feature_col]][value]=\\\n",
    "            splitDataSet(data,best_feature_col,value),subFeature_columns)\n",
    "            \n",
    "    \n",
    "    return myTree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.581826669453059\n"
     ]
    }
   ],
   "source": [
    "print(calculate_entropy(dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "select_best_features(dataset,[1,2,3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vote(np.array([[1],[1],[1],[1],[1],[1],[1],[1],[0],[0],[0],[0],[0],[0],[0],[0],[0],[0],[0],[0],[0],[0],[0]]),[])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'petal length in cm': {'<=3.75866666667': {'petal width in cm': {'<=1.72258064516': {'sepal width in cm': {'<=3.01739130435': {'sepal length in cm ': {'<=6.75555555556': 2.0,\n",
       "        '>=6.75555555556': 2.0}},\n",
       "      '>=3.01739130435': {'sepal length in cm ': {'<=6.54285714286': 2.0,\n",
       "        '>=6.54285714286': 2.0}}}},\n",
       "    '>=1.72258064516': {'sepal width in cm': {'<=2.79361702128': {'sepal length in cm ': {'<=6.24137931034': 1.0,\n",
       "        '>=6.24137931034': 1.0}},\n",
       "      '>=2.79361702128': {'sepal length in cm ': {'<=5.75555555556': 1.0,\n",
       "        '>=5.75555555556': 1.0}}}}}},\n",
       "  '>=3.75866666667': {'petal width in cm': {'<=0.343859649123': {'sepal width in cm': {'<=3.15': {'sepal length in cm ': {}},\n",
       "      '>=3.15': {'sepal length in cm ': {}}}},\n",
       "    '>=0.343859649123': {'sepal length in cm ': {'<=4.95365853659': {'sepal width in cm': {'<=3.58571428571': 0.0,\n",
       "        '>=3.58571428571': 0.0}},\n",
       "      '>=4.95365853659': {'sepal width in cm': {'<=3.115': 0.0,\n",
       "        '>=3.115': 0.0}}}}}}}}"
      ]
     },
     "execution_count": 241,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "create_tree(dataset,features_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 239,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calNotNegative1([1,2,3,4,-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
